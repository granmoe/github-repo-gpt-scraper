[
  {
    "path": ".gitignore",
    "url": "https://github.com/granmoe/gpt-toolkit/blob/main/.gitignore",
    "content": "# Node.js\nnode_modules/\nnpm-debug.log\n\n# TypeScript\ndist/\n*.tsbuildinfo\n\n# IDEs and Editors\n.vscode/\n.idea/\n*.sublime*\n*.swp\n*.swo\n\n# OS-specific\n.DS_Store\nThumbs.db"
  },
  {
    "path": ".npmignore",
    "url": "https://github.com/granmoe/gpt-toolkit/blob/main/.npmignore",
    "content": "src/\n.gitignore\njest.config.ts\ntodo.md\ntsconfig.json\ntsconfig.types.json"
  },
  {
    "path": "README.md",
    "url": "https://github.com/granmoe/gpt-toolkit/blob/main/README.md",
    "content": "# GPT Toolkit\n\nA TypeScript powerhouse for OpenAI's GPT models that wraps the official OpenAI node client. Supercharge your interactions with GPT using this toolkit that not only **makes GPT model responses fully-typed** but also streamlines common needs like retry strategies, trimming tokens when you exceed the max token count, and parsing and then optionally retrying with feedback and a lower temperature 😎\n\n## 🌟 Key Features:\n\n- 🤩 **Fully-Typed LLMs**: Simply pass a `parse` function and get guaranteed type-safe responses back from your OpenAI calls!\n- 🔄 **Retry**: Gracefully handle failures with customizable conditions and exponential backoffs.\n- 🪶 **Token Management**: Ensure your requests always fit the model's token limit by passing a trimTokens function that gives you the overage count and lets you trim tokens in just the right way for your application.\n- 🛡 **Synced with OpenAI's OpenAPI Spec**: Always stay updated with types directly synced to OpenAI's OpenAPI spec and inferred for you.\n\n## 📦 Installation:\n\n```bash\nnpm install gpt-toolkit\n```\n\nor\n\n```bash\npnpm install gpt-toolkit\n```\n\nor\n\n```bash\nyarn add gpt-toolkit\n```\n\n## 🚀 Usage:\n\n### **Experience Fully-Typed LLMs:**\n\n(Note: so far, we only have support for non-streaming chat completion clients - more to come!)\n\nSimply pass a `parse` function and get typed responses back from your OpenAI calls!\n\n```typescript\nimport { createChatClient, ChatCompletion } from 'gpt-toolkit'\n\nconst gptClient = createChatClient({\n  modelId: 'gpt-4',\n  parse: (completion: ChatCompletion) => {\n    if (completion.choices[0].message.content !== null) {\n      const message = completion.choices[0].message.content\n      const lines = message.split('\\n').map(Number)\n\n      if (lines.length === 2 && lines.every((n) => !isNaN(n))) {\n        return lines as [number, number]\n      }\n    }\n  },\n})\n\n// `completion` is of type `[number, number] | undefined` 😎\n// Handle the `undefined` case below, or use the built-in retry function directly within parse! (See examples below)\nconst completion = await gptClient.createCompletion({\n  messages: [\n    {\n      role: 'user',\n      content: 'Please return two numbers from 0-10, one per line',\n    },\n  ],\n})\n```\n\n### **Parse and Retry With Feedback:**\n\n```typescript\nimport { createChatClient, ChatCompletion, Retry } from 'gpt-toolkit'\n\nconst gptClient = createChatClient<ExampleType>({\n  modelId: 'gpt-4',\n  parse: async (completion: ChatCompletion, retry: Retry<ExampleType>) => {\n    try {\n      const json = JSON.parse(completion: ChatCompletion.choices[0].message.content)\n      return json as ExampleType\n    } catch (error) {\n      return retry({\n        feedback: 'Pass any feedback to GPT-4 here!',\n        updatedModelParams: {\n          temperature: 0, // You can turn down temp on retry (and modify any other model params)\n        },\n      })\n    }\n  },\n})\n```\n\n### **Parse and Validate With Zod:**\n\n```typescript\nimport { createChatClient, ChatCompletion, Retry } from 'gpt-toolkit'\nimport { z } from 'zod'\n\n// Define a Zod schema for your expected data structure\nconst ExampleTypeSchema = z.object({\n  title: z.string(),\n  description: z.string(),\n})\n\ntype ExampleType = z.infer<typeof ExampleTypeSchema>\n\nconst gptClient = createChatClient<ExampleType>({\n  modelId: 'gpt-4',\n  parse: async (completion: ChatCompletion, retry: Retry<ExampleType>) => {\n    const text = completion.choices[0].message.content\n\n    try {\n      const parsedData = JSON.parse(text)\n      const validationResult = ExampleTypeSchema.safeParse(parsedData)\n\n      if (validationResult.success) {\n        return validationResult.data\n      } else {\n        const zodErrors = validationResult.error.issues\n          .map((issue) => issue.message)\n          .join(', ')\n        return retry({\n          feedback: `There was a validation error: ${zodErrors}. Please format your response correctly!`,\n        })\n      }\n    } catch (error) {\n      return retry({ feedback: 'Please provide a valid JSON response.' })\n    }\n  },\n})\n```\n\n### **Custom Retry Strategy:**\n\n```typescript\nconst retryStrategy: RetryStrategy = {\n  shouldRetry: (error) => error.status === 500,\n  calculateDelay: (retryCount) => 1000 * Math.max(retryCount, 1),\n  maxRetries: 2,\n}\n\nconst gptClient = createChatClient({\n  modelId: 'gpt-4',\n  retryStrategy,\n})\n```\n\n### **Trim Tokens - Easy and Flexible:**\n\n```typescript\nconst gptClient = createChatClient({\n  modelId: 'gpt-4',\n  // overage is the number of tokens by which you've exceeded the limit\n  trimTokens: (messages, overage) => {\n    // Do whatever you want in here--just return an array of message on your way out!\n    if (messages.length > 1) {\n      return messages.slice(1)\n    }\n\n    return messages\n  },\n  minResponseTokens: 400,\n})\n```\n\n`trimTokens` will be called any time your request messages token count + minResponseTokens (if passed) exceeds the max tokens for your chosen model. The messages you return from it will be sent in the request to OpenAI instead of the original messages.\n\n## 📖 Documentation:\n\nI plan to add more extensive documentation if this gains traction, but for now, the best way to learn is to read the code and the tests.\n\n## 🌱 Contribute:\n\nYour feedback and expertise is most welcome! Share bugs, request features, or contribute directly by submitting a pull request.\n\n## 📝 License:\n\nLicensed under the MIT License.\n\n---\n\n**GPT Toolkit** – Easy type-safe interactions with OpenAI models! 🌌🚀\n"
  },
  {
    "path": "jest.config.ts",
    "url": "https://github.com/granmoe/gpt-toolkit/blob/main/jest.config.ts",
    "content": "import type { JestConfigWithTsJest } from 'ts-jest'\n\nconst jestConfig: JestConfigWithTsJest = {\n  testEnvironment: 'node',\n  transform: {\n    '^.+\\\\.ts$': ['ts-jest', { useESM: true }],\n  },\n}\n\nexport default jestConfig\n"
  },
  {
    "path": "package.json",
    "url": "https://github.com/granmoe/gpt-toolkit/blob/main/package.json",
    "content": "{\n  \"name\": \"gpt-toolkit\",\n  \"version\": \"1.3.4\",\n  \"description\": \"\",\n  \"main\": \"dist/index.js\",\n  \"files\": [\n    \"dist\"\n  ],\n  \"scripts\": {\n    \"test\": \"jest\",\n    \"build\": \"rm -rf ./dist && tsc -b ./tsconfig.json ./tsconfig.types.json\",\n    \"prepublishOnly\": \"npm run build\"\n  },\n  \"keywords\": [],\n  \"author\": \"\",\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"axios\": \"1.5.0\",\n    \"axios-retry\": \"3.7.0\",\n    \"gpt-3-encoder\": \"^1.1.4\",\n    \"openai\": \"^4.11.1\"\n  },\n  \"devDependencies\": {\n    \"@types/jest\": \"^29.5.5\",\n    \"@types/node\": \"^20.6.2\",\n    \"jest\": \"^29.7.0\",\n    \"msw\": \"^1.3.1\",\n    \"ts-jest\": \"^29.1.1\",\n    \"ts-node\": \"^10.9.1\",\n    \"typescript\": \"5.2.2\"\n  }\n}"
  },
  {
    "path": "src/__tests/parse.test.ts",
    "url": "https://github.com/granmoe/gpt-toolkit/blob/main/src/__tests/parse.test.ts",
    "content": "import { ChatCompletion, Retry, createChatClient } from '../create-chat-client'\nimport { rest } from 'msw'\nimport { setupServer } from 'msw/node'\n\nconst server = setupServer()\n\nbeforeAll(() => server.listen())\nafterEach(() => server.resetHandlers())\nafterAll(() => server.close())\n\ndescribe('parse', () => {\n  it('supports parsing JSON', async () => {\n    const testResponse = {\n      choices: [\n        {\n          message: {\n            role: 'assistant',\n            content: JSON.stringify({\n              foo: 1,\n              bar: 2,\n              baz: ['quux'],\n            }),\n          },\n        },\n      ],\n    }\n\n    server.use(\n      rest.post('*', (_req, res, ctx) => {\n        return res(ctx.json(testResponse))\n      }),\n    )\n\n    type ExampleType = {\n      foo: number\n      bar: number\n      baz: string[]\n    }\n\n    const gptClient = createChatClient({\n      modelId: 'gpt-4',\n      parse: (completion: ChatCompletion): ExampleType => {\n        return JSON.parse(completion.choices[0].message.content ?? '')\n      },\n    })\n\n    const result = await gptClient.createCompletion({\n      messages: [\n        {\n          role: 'user',\n          content: 'Test content',\n        },\n      ],\n    })\n\n    expect(result).toEqual({\n      foo: 1,\n      bar: 2,\n      baz: ['quux'],\n    })\n  })\n\n  it('supports retrying with feedback while under max retries (default of 2)', async () => {\n    const testResponse = {\n      choices: [\n        {\n          message: {\n            role: 'assistant',\n            content: JSON.stringify({\n              foo: 1,\n              bar: 2,\n              baz: ['quux'],\n            }),\n          },\n        },\n      ],\n    }\n\n    let callCount = 0\n\n    server.use(\n      rest.post('*', (_req, res, ctx) => {\n        if (callCount < 2) {\n          callCount++\n\n          return res(\n            ctx.json({\n              choices: [\n                {\n                  text: 'This will blow up JSON.parse',\n                },\n              ],\n            }),\n          )\n        }\n\n        return res(ctx.json(testResponse))\n      }),\n    )\n\n    type ExampleType = {\n      foo: number\n      bar: number\n      baz: string[]\n    }\n\n    const gptClient = createChatClient<ExampleType>({\n      modelId: 'gpt-4',\n      parse: async (completion: ChatCompletion, retry: Retry<ExampleType>) => {\n        try {\n          const json = JSON.parse(completion.choices[0].message.content ?? '')\n          return json as ExampleType\n        } catch (error) {\n          return retry({\n            feedback:\n              'Hey, GPT-4! Please return the output in the right format!',\n            updatedModelParams: {\n              temperature: 0,\n            },\n          })\n        }\n      },\n    })\n\n    const result = await gptClient.createCompletion({\n      messages: [\n        {\n          role: 'user',\n          content: 'Test content',\n        },\n      ],\n    })\n\n    expect(result).toEqual({\n      foo: 1,\n      bar: 2,\n      baz: ['quux'],\n    })\n  })\n})\n"
  },
  {
    "path": "src/__tests/retryStrategy.test.ts",
    "url": "https://github.com/granmoe/gpt-toolkit/blob/main/src/__tests/retryStrategy.test.ts",
    "content": "import { RetryStrategy, createChatClient, ChatCompletionMessageParam } from '..'\nimport { rest } from 'msw'\nimport { setupServer } from 'msw/node'\n\nconst server = setupServer()\n\nbeforeAll(() => server.listen())\nafterEach(() => server.resetHandlers())\nafterAll(() => server.close())\n\ndescribe('Retry', () => {\n  it('should retry on status >= 400 by default', async () => {\n    const successResponse = {\n      choices: [{ message: { role: 'assistant', content: 'Test response' } }],\n    }\n\n    let callCount = 0\n    let httpStatuses = [400, 401, 429, 500, 502]\n\n    server.use(\n      rest.post('*', (_req, res, ctx) => {\n        if (callCount < httpStatuses.length - 1) {\n          callCount++\n          return res(ctx.status(httpStatuses[callCount]))\n        }\n\n        return res(ctx.json(successResponse))\n      }),\n    )\n\n    const gptClient = createChatClient({\n      modelId: 'gpt-4',\n      retryStrategy: {\n        maxRetries: 5,\n      },\n    })\n\n    const result = await gptClient.createCompletion({\n      messages: mockMessages,\n    })\n\n    expect(result).toEqual(successResponse.choices[0].message.content)\n  })\n\n  it('should retry with custom strategy', async () => {\n    const successResponse = {\n      choices: [{ message: { role: 'assistant', content: 'Test response' } }],\n    }\n\n    let callCount = 0\n\n    server.use(\n      rest.post('*', (_req, res, ctx) => {\n        if (callCount < 2) {\n          callCount++\n          return res(ctx.status(500))\n        }\n\n        return res(ctx.json(successResponse))\n      }),\n    )\n\n    const retryStrategy: RetryStrategy = {\n      shouldRetry: (error) => error.status === 500,\n      calculateDelay: (retryCount) => 1000 * Math.max(retryCount, 1),\n      maxRetries: 2,\n    }\n\n    const gptClient = createChatClient({\n      modelId: 'gpt-4',\n      retryStrategy,\n    })\n\n    const result = await gptClient.createCompletion({\n      messages: mockMessages,\n    })\n\n    expect(result).toBe(successResponse.choices[0].message.content)\n  })\n\n  it('should fail after max retries exceeded', async () => {\n    server.use(\n      rest.post('*', (_req, res, ctx) => {\n        return res(ctx.status(429))\n      }),\n    )\n\n    const gptClient = createChatClient({\n      modelId: 'gpt-4',\n      retryStrategy: {\n        maxRetries: 1,\n      },\n    })\n\n    const result = gptClient.createCompletion({\n      messages: mockMessages,\n    })\n\n    await expect(result).rejects.toThrow()\n  })\n})\n\nconst mockMessages: ChatCompletionMessageParam[] = [\n  {\n    role: 'user',\n    content: 'Test content',\n  },\n]\n"
  },
  {
    "path": "src/__tests/trimTokens.test.ts",
    "url": "https://github.com/granmoe/gpt-toolkit/blob/main/src/__tests/trimTokens.test.ts",
    "content": "import {\n  createChatClient,\n  ChatCompletionMessageParam,\n} from '../create-chat-client'\nimport { rest } from 'msw'\nimport { setupServer } from 'msw/node'\n\nconst server = setupServer()\n\nbeforeAll(() => server.listen())\nafterEach(() => server.resetHandlers())\nafterAll(() => server.close())\n\ndescribe('trimTokens', () => {\n  it('trims messages when token count exceeds maximum allowed for the model', async () => {\n    let requestMessages\n\n    server.use(\n      rest.post('*', async (req, res, ctx) => {\n        requestMessages = (await req?.json()).messages\n        return res(\n          ctx.json({\n            choices: [\n              {\n                message: {\n                  role: 'assistant',\n                  content: 'This is a test response',\n                },\n              },\n            ],\n          }),\n        )\n      }),\n    )\n\n    const gptClient = createChatClient({\n      modelId: 'gpt-4',\n      trimTokens: (messages, _overage) => messages.slice(1), // Drop first message\n    })\n\n    const messages: ChatCompletionMessageParam[] = [\n      MESSAGE_WITH_8192_TOKENS,\n      {\n        role: 'user',\n        content: 'This is a normal message',\n      },\n    ]\n\n    const result = await gptClient.createCompletion({\n      messages,\n    })\n\n    expect(requestMessages).toEqual(messages.slice(1))\n    expect(result).toBe('This is a test response')\n  })\n\n  it('takes into account minResponseTokens if passed', async () => {\n    let requestMessages\n\n    server.use(\n      rest.post('*', async (req, res, ctx) => {\n        requestMessages = (await req?.json()).messages\n        return res(\n          ctx.json({\n            choices: [\n              {\n                message: {\n                  role: 'assistant',\n                  content: 'This is a test response',\n                },\n              },\n            ],\n          }),\n        )\n      }),\n    )\n\n    const gptClient = createChatClient({\n      modelId: 'gpt-3.5-turbo',\n      trimTokens: (messages, _overage) => messages.slice(1), // Drop oldest message\n      minResponseTokens: 3098,\n    })\n\n    // Since gpt-3.5-turbo allows 4097 tokens, and we have min response tokens of 3097, we have 999 left to work with\n    const messages: ChatCompletionMessageParam[] = [\n      {\n        role: 'user',\n        content: SINGLE_TOKEN_WORD.repeat(100 - 6),\n      },\n      {\n        role: 'user',\n        content: SINGLE_TOKEN_WORD.repeat(900 - 6),\n      },\n    ]\n\n    // Our trim tokens strategy will drop the oldest messages\n    // so we should end up calling OpenAI with just the second message and get a successful response\n\n    const result = await gptClient.createCompletion({\n      messages,\n    })\n\n    expect(requestMessages).toEqual(messages.slice(1))\n    expect(result).toBe('This is a test response')\n  })\n\n  it('throws an error when messages still exceed the token limit even after trimming', async () => {\n    server.use(\n      rest.post('*', (_req, res, ctx) => {\n        return res(\n          ctx.json({\n            choices: [\n              {\n                message: {\n                  role: 'assistant',\n                  content: 'This is a test response',\n                },\n              },\n            ],\n          }),\n        )\n      }),\n    )\n\n    const gptClient = createChatClient({\n      modelId: 'gpt-4',\n      trimTokens: (messages, _overage) => messages, // don't drop any tokens\n    })\n\n    const messages: ChatCompletionMessageParam[] = [\n      MESSAGE_WITH_8192_TOKENS,\n      MESSAGE_WITH_8192_TOKENS,\n    ]\n\n    await expect(\n      gptClient.createCompletion({\n        messages,\n      }),\n    ).rejects.toThrow(\n      /Token count \\(\\d+\\) exceeds max tokens per request \\(8192\\)/,\n    )\n  })\n\n  it('throws an error when messages still exceed the token limit even after trimming, taking into account minResponseTokens', async () => {\n    server.use(\n      rest.post('*', (_req, res, ctx) => {\n        return res(\n          ctx.json({\n            choices: [\n              {\n                message: {\n                  role: 'assistant',\n                  content: 'This is a test response',\n                },\n              },\n            ],\n          }),\n        )\n      }),\n    )\n\n    const gptClient = createChatClient({\n      modelId: 'gpt-4',\n      trimTokens: (messages, _overage) => {\n        if (messages.length > 1) {\n          return messages.slice(1)\n        }\n\n        return messages\n      },\n      minResponseTokens: 6000,\n    })\n\n    const messages: ChatCompletionMessageParam[] = [MESSAGE_WITH_8192_TOKENS]\n\n    await expect(\n      gptClient.createCompletion({\n        messages,\n      }),\n    ).rejects.toThrow(\n      /Token count \\(\\d+\\) exceeds max tokens per request \\(8192\\)/,\n    )\n  })\n\n  it('messages are not trimmed if trimTokens is not passed', async () => {\n    server.use(\n      rest.post('*', async (req, res, ctx) => {\n        const requestJson = await req.json()\n        if ((requestJson.messages ?? []).length > 1) {\n          return res(ctx.status(400))\n        } else {\n          // Shouldn't get here\n          return res(\n            ctx.json({\n              choices: [\n                {\n                  message: {\n                    role: 'assistant',\n                    content: 'This is a test response',\n                  },\n                },\n              ],\n            }),\n          )\n        }\n      }),\n    )\n\n    const gptClient = createChatClient({\n      modelId: 'gpt-4',\n    })\n\n    const messages: ChatCompletionMessageParam[] = [\n      MESSAGE_WITH_8192_TOKENS,\n      MESSAGE_WITH_8192_TOKENS,\n    ]\n\n    await expect(\n      gptClient.createCompletion({\n        messages,\n      }),\n    ).rejects.toThrow('400 status code (no body)')\n  })\n})\n\nconst SINGLE_TOKEN_WORD = 'hey'\n\n// The formatting takes up 7 tokens\nconst MESSAGE_WITH_8192_TOKENS: ChatCompletionMessageParam = {\n  role: 'user',\n  content: 'hey' + SINGLE_TOKEN_WORD.repeat(8185),\n}\n"
  },
  {
    "path": "src/create-chat-client.ts",
    "url": "https://github.com/granmoe/gpt-toolkit/blob/main/src/create-chat-client.ts",
    "content": "import { encode } from 'gpt-3-encoder'\nimport OpenAI from 'openai'\n\nexport type ChatCompletion = OpenAI.Chat.Completions.ChatCompletion\n\nexport type ChatCompletionMessageParam =\n  OpenAI.Chat.Completions.ChatCompletionMessageParam\n\nexport type ModelParams = Omit<\n  OpenAI.Chat.Completions.ChatCompletionCreateParamsNonStreaming,\n  'model' | 'messages'\n>\n\n// DEFAULT PARSER\nexport function createChatClient(\n  params: CreateChatClientWithDefaultParserParams,\n): {\n  createCompletion(request: {\n    messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[]\n    functions?: OpenAI.Chat.ChatCompletionCreateParams.Function[]\n    modelParams?: ModelParams\n  }): Promise<string | null>\n}\n\n// CUSTOM PARSER WITHOUT RETRY\nexport function createChatClient<TParsedResponse>(\n  params: CreateChatClientWithCustomParserParams<\n    ResponseParserWithoutRetry<TParsedResponse>\n  >,\n): {\n  createCompletion(request: {\n    messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[]\n    functions?: OpenAI.Chat.ChatCompletionCreateParams.Function[]\n    modelParams?: ModelParams\n  }): Promise<TParsedResponse>\n}\n\n// CUSTOM PARSER WITH RETRY\nexport function createChatClient<TParsedResponse>(\n  params: CreateChatClientWithCustomParserParams<\n    ResponseParserWithRetry<TParsedResponse>\n  >,\n): {\n  createCompletion(request: {\n    messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[]\n    functions?: OpenAI.Chat.ChatCompletionCreateParams.Function[]\n    modelParams?: ModelParams\n  }): Promise<TParsedResponse>\n}\n\nexport function createChatClient<TParsedResponse>(\n  params:\n    | CreateChatClientWithCustomParserParams<\n        ResponseParserWithRetry<TParsedResponse>\n      >\n    | CreateChatClientWithCustomParserParams<\n        ResponseParserWithoutRetry<TParsedResponse>\n      >\n    | CreateChatClientWithDefaultParserParams,\n) {\n  const openAiApiKey = params.apiKey ?? process.env.OPENAI_API_KEY\n  if (!openAiApiKey) {\n    throw new Error(\n      'OpenAI API key is required. You must either pass apiKey or set the OPENAI_API_KEY environment variable.',\n    )\n  }\n\n  const openAiClient = new OpenAI({\n    apiKey: openAiApiKey,\n    maxRetries: 0,\n  })\n\n  const retryDelay = (retryCount: number) => {\n    if (IS_TEST) {\n      return 0\n    }\n\n    // Return time to delay in milliseconds\n    return params.retryStrategy?.calculateDelay\n      ? params.retryStrategy.calculateDelay(retryCount)\n      : 2 ** retryCount * 1000 + // Exponential backoff: 1 second, then 2, 4, 8 etc\n          (Math.floor(Math.random() * 50) + 1) // Additional random delay between 1 and 50 ms\n  }\n\n  const shouldRetry = (error: any) => {\n    return params.retryStrategy?.shouldRetry\n      ? params.retryStrategy.shouldRetry(error)\n      : !!error.status && error.status >= 400\n  }\n\n  const updateModelParamsForRetry = (modelParams: ModelParams) => {\n    if (!params.retryStrategy?.updateModelParams) {\n      return modelParams\n    }\n\n    return params.retryStrategy.updateModelParams(modelParams)\n  }\n\n  const callOpenAiWithRetry = (\n    request: {\n      messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[]\n      functions?: OpenAI.Chat.ChatCompletionCreateParams.Function[]\n      modelParams?: ModelParams\n    },\n    options?: Parameters<typeof openAiClient.chat.completions.create>[1],\n  ) => {\n    let retryCount = 0\n\n    async function makeRequest(modelParams: ModelParams) {\n      try {\n        const body = {\n          model: params.modelId,\n          messages: request.messages,\n          functions: request.functions,\n          ...modelParams,\n        }\n\n        return await openAiClient.chat.completions.create(body, options)\n      } catch (err) {\n        if (!(err instanceof OpenAI.APIError) && !IS_TEST) {\n          throw err\n        }\n\n        if (retryCount >= (params.retryStrategy?.maxRetries ?? 2)) {\n          throw err\n        }\n\n        if (shouldRetry(err)) {\n          retryCount++\n\n          const delay = retryDelay(retryCount)\n          await new Promise((r) => setTimeout(r, delay))\n\n          const updatedModelParams = updateModelParamsForRetry(modelParams)\n\n          return makeRequest(updatedModelParams)\n        }\n\n        throw err\n      }\n    }\n\n    const modelParamsForInitialRequest: ModelParams = {\n      ...params.modelDefaultParams,\n      ...request.modelParams,\n    }\n\n    return makeRequest(modelParamsForInitialRequest)\n  }\n\n  if ('parse' in params) {\n    if (isResponseParserWithRetry<TParsedResponse>(params.parse)) {\n      return createChatClientWithCustomParserWithRetry<TParsedResponse>(\n        params as CreateChatClientWithCustomParserParams<\n          ResponseParserWithRetry<TParsedResponse>\n        >,\n        callOpenAiWithRetry,\n      )\n    } else {\n      return createChatClientWithCustomParserWithoutRetry<TParsedResponse>(\n        params as CreateChatClientWithCustomParserParams<\n          ResponseParserWithoutRetry<TParsedResponse>\n        >,\n        callOpenAiWithRetry,\n      )\n    }\n  }\n\n  return createChatClientWithDefaultParser(params, callOpenAiWithRetry)\n}\n\nexport function createChatClientWithCustomParserWithRetry<TParsedResponse>(\n  params: CreateChatClientWithCustomParserParams<\n    ResponseParserWithRetry<TParsedResponse>\n  >,\n  callOpenAiWithRetry: CallOpenAiWithRetry,\n) {\n  const {\n    modelId,\n    modelDefaultParams = {\n      temperature: 0,\n      top_p: 1,\n      frequency_penalty: 0,\n      presence_penalty: 0,\n    },\n    parse,\n    trimTokens,\n    minResponseTokens = 0,\n  } = params\n\n  const tokenTrimmer = trimTokens ? makeTokenTrimmer(trimTokens) : null\n\n  const createCompletion = async (\n    request: {\n      messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[]\n      functions?: OpenAI.Chat.ChatCompletionCreateParams.Function[]\n      modelParams?: ModelParams\n    },\n    __parseRetryCount = 0,\n  ): Promise<TParsedResponse> => {\n    const { messages, functions, modelParams } = request\n\n    const trimmedMessages = tokenTrimmer\n      ? tokenTrimmer(messages, modelId, minResponseTokens)\n      : messages\n\n    const chatCompletion = await callOpenAiWithRetry({\n      messages: trimmedMessages,\n      functions,\n      ...modelDefaultParams,\n      ...modelParams,\n    })\n\n    if (chatCompletion.choices.length === 0) {\n      throw new Error('Empty response from OpenAI')\n    }\n\n    const retry = ({\n      feedback,\n      updatedModelParams,\n    }: {\n      feedback?: string\n      updatedModelParams?: ModelParams\n    }) => {\n      const feedbackMessage: OpenAI.Chat.Completions.ChatCompletionMessageParam =\n        {\n          role: 'system',\n          content: feedback ?? '',\n        }\n\n      const newRequest = {\n        messages: feedback ? [...messages, feedbackMessage] : messages,\n        modelParams: updatedModelParams\n          ? { ...modelParams, ...updatedModelParams }\n          : modelParams,\n      }\n\n      return createCompletion(newRequest, __parseRetryCount + 1)\n    }\n\n    const parsedResponse = await parse(chatCompletion, retry)\n\n    return parsedResponse\n  }\n\n  return {\n    createCompletion,\n  }\n}\n\nexport function createChatClientWithCustomParserWithoutRetry<TParsedResponse>(\n  params: CreateChatClientWithCustomParserParams<\n    ResponseParserWithoutRetry<TParsedResponse>\n  >,\n  callOpenAiWithRetry: CallOpenAiWithRetry,\n) {\n  const {\n    modelId,\n    modelDefaultParams = {\n      temperature: 0,\n      top_p: 1,\n      frequency_penalty: 0,\n      presence_penalty: 0,\n    },\n    parse,\n    trimTokens,\n    minResponseTokens = 0,\n  } = params\n\n  const tokenTrimmer = trimTokens ? makeTokenTrimmer(trimTokens) : null\n\n  const createCompletion = async (\n    request: {\n      messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[]\n      functions?: OpenAI.Chat.ChatCompletionCreateParams.Function[]\n      modelParams?: ModelParams\n    },\n    __parseRetryCount = 0,\n  ): Promise<TParsedResponse> => {\n    const { messages, functions, modelParams } = request\n\n    const trimmedMessages = tokenTrimmer\n      ? tokenTrimmer(messages, modelId, minResponseTokens)\n      : messages\n\n    const chatCompletion = await callOpenAiWithRetry({\n      messages: trimmedMessages,\n      functions,\n      ...modelDefaultParams,\n      ...modelParams,\n    })\n\n    if (chatCompletion.choices.length === 0) {\n      throw new Error('Empty response from OpenAI')\n    }\n\n    return parse(chatCompletion)\n  }\n\n  return {\n    createCompletion,\n  }\n}\n\nexport function createChatClientWithDefaultParser(\n  params: CreateChatClientWithDefaultParserParams,\n  callOpenAiWithRetry: CallOpenAiWithRetry,\n) {\n  const {\n    modelId,\n    modelDefaultParams = {\n      temperature: 0,\n      top_p: 1,\n      frequency_penalty: 0,\n      presence_penalty: 0,\n    },\n    trimTokens,\n    minResponseTokens = 0,\n  } = params\n\n  const tokenTrimmer = trimTokens ? makeTokenTrimmer(trimTokens) : null\n\n  const createCompletion = async (\n    request: {\n      messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[]\n      modelParams?: ModelParams\n    },\n    __parseRetryCount = 0,\n  ): Promise<string | null> => {\n    const { messages, modelParams } = request\n\n    const trimmedMessages = tokenTrimmer\n      ? tokenTrimmer(messages, modelId, minResponseTokens)\n      : messages\n\n    const chatCompletion = await callOpenAiWithRetry({\n      messages: trimmedMessages,\n      ...modelDefaultParams,\n      ...modelParams,\n    })\n\n    if (chatCompletion.choices.length === 0) {\n      throw new Error('Empty response from OpenAI')\n    }\n\n    return chatCompletion.choices[0].message.content\n  }\n\n  return {\n    createCompletion,\n  }\n}\n\nconst makeTokenTrimmer =\n  (trimTokens: TrimTokens) =>\n  (\n    originalMessages: OpenAI.Chat.Completions.ChatCompletionMessageParam[],\n    modelId: string,\n    minResponseTokens: number,\n  ): OpenAI.Chat.Completions.ChatCompletionMessageParam[] => {\n    const maxTokensPerRequest = maxTokensByModelId[modelId]\n    if (maxTokensPerRequest) {\n      const tokenCount =\n        getTokenCountForMessages(originalMessages) + minResponseTokens\n\n      if (tokenCount > maxTokensPerRequest) {\n        const overage = tokenCount - maxTokensPerRequest\n\n        const trimmedMessages = trimTokens(originalMessages, overage)\n\n        const updatedTokenCount =\n          getTokenCountForMessages(trimmedMessages) + minResponseTokens\n\n        if (updatedTokenCount > maxTokensPerRequest) {\n          throw new Error(\n            `Token count (${tokenCount}) exceeds max tokens per request (${maxTokensPerRequest})`,\n          )\n        }\n\n        return trimmedMessages\n      }\n    }\n\n    return originalMessages\n  }\n\nexport type CreateChatClientWithCustomParserParams<TResponseParser> = {\n  apiKey?: string\n  modelId: string\n  modelDefaultParams?: ModelParams\n  parse: TResponseParser\n  trimTokens?: TrimTokens\n  minResponseTokens?: number\n  retryStrategy?: {\n    shouldRetry?: (error: InstanceType<typeof OpenAI.APIError>) => boolean\n    calculateDelay?: (retryCount: number) => number\n    maxRetries?: number\n    updateModelParams?: (modelParams: ModelParams) => ModelParams\n  }\n}\n\nexport type TrimTokens = (\n  messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[],\n  overage: number,\n) => OpenAI.Chat.Completions.ChatCompletionMessageParam[]\n\nexport type CreateChatClientWithDefaultParserParams = Omit<\n  CreateChatClientWithCustomParserParams<any>,\n  'parse'\n>\n\nexport type RetryStrategy =\n  CreateChatClientWithCustomParserParams<any>['retryStrategy']\n\ntype ResponseParserWithRetry<T> = (\n  response: OpenAI.Chat.Completions.ChatCompletion,\n  retry: Retry<T>,\n) => Promise<T>\n\nexport type Retry<T> = ({\n  feedback,\n  updatedModelParams,\n}: {\n  feedback?: string\n  updatedModelParams?: ModelParams\n}) => Promise<T>\n\n// TODO: Find a way to get Core.RequestOptions out of 'openai' package\ntype HTTPMethod = 'get' | 'post' | 'put' | 'patch' | 'delete'\ntype Headers = Record<string, string | null | undefined>\n\ntype OpenAiCoreRequestOptions<Req extends {} = Record<string, unknown>> = {\n  method?: HTTPMethod\n  path?: string\n  query?: Req\n  body?: Req\n  headers?: Headers\n  maxRetries?: number\n  stream?: boolean\n  timeout?: number\n  httpAgent?: any\n  signal?: AbortSignal\n  idempotencyKey?: string\n}\n\ntype CallOpenAiWithRetry = (\n  request: {\n    messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[]\n    functions?: OpenAI.Chat.ChatCompletionCreateParams.Function[]\n    modelParams?: ModelParams\n  },\n  options?: OpenAiCoreRequestOptions,\n) => Promise<OpenAI.Chat.Completions.ChatCompletion>\n\ntype ResponseParserWithoutRetry<T = string> = (\n  response: OpenAI.Chat.Completions.ChatCompletion,\n) => T\n\nfunction isResponseParserWithRetry<T>(\n  parse: any,\n): parse is ResponseParserWithRetry<T> {\n  return parse.length === 2\n}\n\n// A TypeScript adaptation of https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n// They count a fixed number of tokens to account for the formatting the model uses, but we instead first format the messages the way\n// the model will,  and then we count the total tokens for this formatted string.\n// We also don't yet support the \"name\" field.\n// They also account for replies, but we do this differently: we have the caller pass in minReponseTokens (optionally)\n//  and make sure that request tokens + minResponseTokens < maxTokensPerRequest\nconst getTokenCountForMessages = (\n  messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[],\n) =>\n  encode(\n    messages\n      .map((m) => [`role: ${m.role}`, `content: ${m.content}`].join('\\n'))\n      .join('\\n'),\n  ).length\n\nconst maxTokensByModelId: Record<string, number> = {\n  'gpt-3.5-turbo': 4097,\n  'gpt-3.5-turbo-0301': 4097,\n  'gpt-3.5-turbo-0613': 4097,\n  'gpt-3.5-turbo-16k': 16385,\n  'gpt-3.5-turbo-16k-0613': 16385,\n  'gpt-4': 8192,\n  'gpt-4-0314': 8192,\n  'gpt-4-0613': 8192,\n  'gpt-4-32k': 32768,\n  'gpt-4-32k-0314': 32768,\n  'gpt-4-32k-0613': 32768,\n}\n\nconst IS_TEST = process.env.NODE_ENV === 'test'\n"
  },
  {
    "path": "src/index.ts",
    "url": "https://github.com/granmoe/gpt-toolkit/blob/main/src/index.ts",
    "content": "export * from './create-chat-client'\nexport { default as OpenAI } from 'openai'\n"
  },
  {
    "path": "todo.md",
    "url": "https://github.com/granmoe/gpt-toolkit/blob/main/todo.md",
    "content": "- Make default parser aware of function calling\n\n# Tests\n\n- more retry scenarios\n  - updateModelParams\n  - assert number of HTTP calls and failed etc\n- parse\n  - Zod example\n  - updateModelParams\n  - custom maxParseRetries\n- trimTokens overage\n- pass in various model params\n- e2e / scenarios that combine functionality tested in existing tests (retry + parse + trimTokens etc)\n\n# Future\n\n- Make trimTokens account for functions passed in OpenAI request\n- Make retry aware of delay time in OpenAI response and incorporate \"jitter\" and the other stuff the official client does\n- Consider throwing custom error from parse instead? Would make types cleaner. Then we only retry if it’s custom error, so people can opt out by just not throwing that error.\n- Support \"name\" in messages and token counting\n- Allow passing in custom maxTokensPerRequest (for custom (e.g. fine-tuned) models)\n  - Throw error if people override a known model's maxTokensPerRequest tho\n- Support more OpenAI models and APIs (not just chat)\n  - Support streaming\n\n# Misc\n\n- Cool logo\n- Get added to this https://platform.openai.com/docs/libraries/community-libraries\n"
  },
  {
    "path": "tsconfig.json",
    "url": "https://github.com/granmoe/gpt-toolkit/blob/main/tsconfig.json",
    "content": "{\n  \"compilerOptions\": {\n    \"target\": \"ES2020\",\n    \"lib\": [\n      \"ES2022\",\n      \"DOM\"\n    ],\n    \"module\": \"NodeNext\",\n    \"outDir\": \"./dist\",\n    \"baseUrl\": \"./src\",\n    \"skipLibCheck\": true,\n    \"esModuleInterop\": true,\n    \"allowSyntheticDefaultImports\": true,\n    \"moduleResolution\": \"NodeNext\",\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true,\n    \"strict\": true,\n    \"noUnusedLocals\": true,\n    \"noUnusedParameters\": true,\n    \"noFallthroughCasesInSwitch\": true\n  },\n  \"include\": [\n    \"src\"\n  ],\n  \"exclude\": [\n    \"src/**/*.test.ts\"\n  ]\n}"
  },
  {
    "path": "tsconfig.types.json",
    "url": "https://github.com/granmoe/gpt-toolkit/blob/main/tsconfig.types.json",
    "content": "{\n  \"extends\": \"./tsconfig.json\",\n  \"compilerOptions\": {\n    \"outDir\": \"./dist\",\n    \"declaration\": true,\n    \"emitDeclarationOnly\": true\n  }\n}"
  }
]